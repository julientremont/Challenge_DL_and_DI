# ğŸš€ Challenge Data Engineering - Pipeline d'Analyse du MarchÃ© Tech EuropÃ©en

## ğŸ‘¥ Ã‰quipe du Projet
- **Ethan TOMASO** - ethan.tomaso@efrei.net
- **Antoine VANDEPLANQUE** - antoine.vandeplanque@efrei.net  
- **Elliot FESQUET** - elliot.fesquet@efrei.net
- **Julien TREMONT-RAIMI** - julien.tremont-raimi@efrei.net

**Ã‰cole** : EFREI Paris  
**MatiÃ¨re** : Data Engineering & Data Integration

---

## ğŸ“‹ PrÃ©sentation du Projet

### Objectif
CrÃ©er un pipeline de donnÃ©es complet pour analyser le marchÃ© technologique europÃ©en en collectant, transformant et analysant des donnÃ©es provenant de 6 sources diffÃ©rentes.

### Architecture
Pipeline moderne suivant l'architecture **Medallion** (Bronze â†’ Silver â†’ Gold) avec API REST complÃ¨te.

```
ğŸ¥‰ Bronze Layer     â†’     ğŸ¥ˆ Silver Layer     â†’     ğŸ¥‡ Gold Layer
DonnÃ©es Brutes            DonnÃ©es NettoyÃ©es         API & Analytics
```

### Sources de DonnÃ©es IntÃ©grÃ©es
- ğŸ“Š **Google Trends** - Tendances de recherche technologiques
- ğŸ™ **GitHub Repositories** - PopularitÃ© des projets open source  
- ğŸ“‹ **StackOverflow Survey** - EnquÃªtes dÃ©veloppeurs (2021-2024)
- ğŸ’¼ **Adzuna Jobs API** - DonnÃ©es emploi et salaires europÃ©ens
- ğŸ‡ªğŸ‡º **EuroTechJobs** - Offres d'emploi tech europÃ©ennes
- ğŸŒ **Jobicy API** - Emplois remote europÃ©ens

### Technologies UtilisÃ©es
- **Backend** : Python, Django REST Framework
- **Big Data** : Apache Spark (PySpark), Pandas
- **Base de DonnÃ©es** : MySQL avec pooling de connexions
- **Documentation** : Swagger UI, Postman Collection
- **Architecture** : Microservices, API REST

---

## ğŸ› ï¸ Installation et Configuration

### 1. PrÃ©requis SystÃ¨me

```bash
# Python 3.9+
python --version

# MySQL 8.0+
mysql --version

# Apache Spark 3.4+ (optionnel pour certaines sources)
```

### 2. Installation des DÃ©pendances

```bash
# Cloner le projet
git clone <votre-repo>
cd Challenge_DL_and_DI

# Installer les dÃ©pendances Python
pip install -r requirements.txt
```

### 3. Configuration Base de DonnÃ©es

**CrÃ©er les bases de donnÃ©es MySQL :**

```sql
-- Se connecter Ã  MySQL
mysql -u root -p

-- CrÃ©er les bases de donnÃ©es
CREATE DATABASE silver;
CREATE DATABASE gold;

-- CrÃ©er un utilisateur dÃ©diÃ© (recommandÃ©)
CREATE USER 'tatane'@'localhost' IDENTIFIED BY 'tatane';
GRANT ALL PRIVILEGES ON silver.* TO 'tatane'@'localhost';
GRANT ALL PRIVILEGES ON gold.* TO 'tatane'@'localhost';
FLUSH PRIVILEGES;
```

### 4. Configuration des Variables d'Environnement

**CrÃ©er le fichier `.env` Ã  la racine du projet :**

```bash
# Copier cet exemple dans votre fichier .env
# ===========================================

# Configuration Base de DonnÃ©es
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=silver
MYSQL_USER=tatane
MYSQL_PASSWORD=tatane

# Configuration Spark (optionnel)
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=8g
SPARK_EXECUTOR_CORES=2
SPARK_UI_ENABLED=true
SPARK_UI_PORT=4040

# Configuration Django
DJANGO_SECRET_KEY=your-secret-key-here
DJANGO_DEBUG=True
DJANGO_ALLOWED_HOSTS=localhost,127.0.0.1

# APIs ClÃ©s (optionnel pour certaines sources)
ADZUNA_APP_ID=your_adzuna_app_id
ADZUNA_APP_KEY=your_adzuna_app_key
```

### 5. VÃ©rification de la Configuration

**Tester la connexion MySQL :**

```bash
# Test de connexion basique
python -c "
import os
from dotenv import load_dotenv
import mysql.connector

load_dotenv()
try:
    conn = mysql.connector.connect(
        host=os.getenv('MYSQL_HOST'),
        user=os.getenv('MYSQL_USER'),
        password=os.getenv('MYSQL_PASSWORD'),
        database=os.getenv('MYSQL_DATABASE')
    )
    print('âœ… Connexion MySQL rÃ©ussie!')
    conn.close()
except Exception as e:
    print(f'âŒ Erreur de connexion: {e}')
"
```

---

## ğŸš€ Guide de Lancement

### 1. Collecte des DonnÃ©es (Bronze Layer)

```bash
# GitHub Repositories (Production Ready)
python src/bronze/import_github_repos.py

# StackOverflow Survey (Production Ready)  
python src/bronze/import_stackoverflow_survey.py

# EuroTechJobs (Production Ready)
python src/bronze/import_eurotechjobs.py

# Jobicy Jobs (Production Ready)
python src/bronze/import_jobicy.py

# Google Trends (Ã€ debugger)
python src/bronze/ImportGtrends.py

# Adzuna Jobs (Ã€ debugger)
python src/bronze/ImportAzuna.py
```

### 2. Transformation des DonnÃ©es (Silver Layer)

```bash
# Processeurs Production Ready
python src/silver/process_github_repos.py
python src/silver/process_stackoverflow_survey.py  
python src/silver/process_eurotechjobs.py
python src/silver/JobicySilver.py

# Processeurs Ã  debugger
python src/silver/GtrendsSilver.py
python src/silver/AzunaSilver.py
```

### 3. Lancement de l'API Django

```bash
# DÃ©marrer le serveur Django
python manage.py runserver

# L'API sera accessible sur :
# http://localhost:8000/api/
```

### 4. AccÃ¨s aux Endpoints

**Interface principale :**
- ğŸ  **API Root** : http://localhost:8000/api/
- ğŸ“š **Swagger UI** : http://localhost:8000/api/docs/
- ğŸ“– **ReDoc** : http://localhost:8000/api/redoc/

**Endpoints par source de donnÃ©es :**

```bash
# GitHub Repositories (6 endpoints)
curl http://localhost:8000/api/github-repos/
curl http://localhost:8000/api/github-repos/summary/

# StackOverflow Survey (6 endpoints)  
curl http://localhost:8000/api/stackoverflow-survey/
curl http://localhost:8000/api/stackoverflow-survey/summary/

# EuroTechJobs (6 endpoints)
curl http://localhost:8000/api/eurotechjobs/
curl http://localhost:8000/api/eurotechjobs/summary/

# Jobicy Jobs (8 endpoints) - LE PLUS COMPLET
curl http://localhost:8000/api/jobicy-jobs/
curl http://localhost:8000/api/jobicy-jobs/summary/
curl http://localhost:8000/api/jobicy-jobs/by-country/
curl http://localhost:8000/api/jobicy-jobs/company-analysis/

# Google Trends (6 endpoints)
curl http://localhost:8000/api/trends/
curl http://localhost:8000/api/trends/summary/

# Adzuna Jobs (5 endpoints)
curl http://localhost:8000/api/adzuna-jobs/
curl http://localhost:8000/api/adzuna-jobs/summary/
```

---

## ğŸ“Š Ã‰tat d'ImplÃ©mentation

### âœ… Production Ready (60% du projet)
- **ğŸ™ GitHub Repositories** : Pipeline complet + API (6 endpoints)
- **ğŸ“‹ StackOverflow Survey** : 4 annÃ©es de donnÃ©es + API (6 endpoints)  
- **ğŸ‡ªğŸ‡º EuroTechJobs** : Scraping + API (6 endpoints)
- **ğŸŒ Jobicy Jobs** : API complÃ¨te (8 endpoints)

### ğŸ”§ Ã€ Debugger (40% du projet)
- **ğŸ“Š Google Trends** : API prÃªte, pipeline bronze/silver Ã  corriger
- **ğŸ’¼ Adzuna Jobs** : API prÃªte, clÃ©s API et flux de donnÃ©es Ã  finaliser

### ğŸ› ï¸ Infrastructure (100% ComplÃ¨te)
- âœ… **SparkManager** : Gestion centralisÃ©e des sessions Spark
- âœ… **SQLManager** : Connexions MySQL avec pooling automatique
- âœ… **MySQLSchemas** : SchÃ©mas centralisÃ©s pour toutes les tables
- âœ… **PathManager** : Gestion uniforme des chemins de fichiers
- âœ… **Django API** : 37 endpoints avec documentation Swagger

---

## ğŸ“ Structure du Projet

```
Challenge_DL_and_DI/
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ bronze/           # Collecte donnÃ©es brutes
â”‚   â”‚   â”œâ”€â”€ import_github_repos.py      âœ… Production
â”‚   â”‚   â”œâ”€â”€ import_stackoverflow_survey.py  âœ… Production  
â”‚   â”‚   â”œâ”€â”€ import_eurotechjobs.py       âœ… Production
â”‚   â”‚   â”œâ”€â”€ import_jobicy.py             âœ… Production
â”‚   â”‚   â”œâ”€â”€ ImportGtrends.py             ğŸ”§ Ã€ debugger
â”‚   â”‚   â””â”€â”€ ImportAzuna.py               ğŸ”§ Ã€ debugger
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ silver/           # Transformation donnÃ©es
â”‚   â”‚   â”œâ”€â”€ process_github_repos.py     âœ… Production
â”‚   â”‚   â”œâ”€â”€ process_stackoverflow_survey.py  âœ… Production
â”‚   â”‚   â”œâ”€â”€ process_eurotechjobs.py      âœ… Production
â”‚   â”‚   â”œâ”€â”€ JobicySilver.py              âœ… Production
â”‚   â”‚   â”œâ”€â”€ GtrendsSilver.py             ğŸ”§ Ã€ debugger
â”‚   â”‚   â””â”€â”€ AzunaSilver.py               ğŸ”§ Ã€ debugger
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ api/              # API REST Django
â”‚   â”‚   â”œâ”€â”€ github_jobs/     # 6 endpoints âœ…
â”‚   â”‚   â”œâ”€â”€ stackoverflow_survey/  # 6 endpoints âœ…
â”‚   â”‚   â”œâ”€â”€ eurotechjobs/    # 6 endpoints âœ…
â”‚   â”‚   â”œâ”€â”€ jobicy_jobs/     # 8 endpoints âœ…
â”‚   â”‚   â”œâ”€â”€ trends/          # 6 endpoints âœ…
â”‚   â”‚   â””â”€â”€ adzuna_jobs/     # 5 endpoints âœ…
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/            # Utilitaires
â”‚       â”œâ”€â”€ sparkmanager.py     âœ… Production
â”‚       â”œâ”€â”€ sqlmanager.py       âœ… Production  
â”‚       â”œâ”€â”€ mysql_schemas.py    âœ… Production
â”‚       â””â”€â”€ paths.py             âœ… Production
â”‚
â”œâ”€â”€ ğŸ“‚ data/                 # DonnÃ©es (crÃ©Ã© automatiquement)
â”‚   â”œâ”€â”€ bronze/             # DonnÃ©es brutes
â”‚   â”œâ”€â”€ silver/             # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ gold/               # DonnÃ©es analysÃ©es
â”‚
â”œâ”€â”€ .env                    # Variables d'environnement
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ manage.py              # Django management
â””â”€â”€ POSTMAN_API.json       # Collection Postman complÃ¨te
```

---

## ğŸ§ª Tests et Validation

### Tester l'API

```bash
# Test simple de l'API
curl http://localhost:8000/api/

# Test avec filtres (Jobicy - le plus complet)
curl "http://localhost:8000/api/jobicy-jobs/?country_code=fr&has_salary=true"

# Test analytics avancÃ©es
curl http://localhost:8000/api/jobicy-jobs/company-analysis/
```

### VÃ©rifier les DonnÃ©es

```bash
# VÃ©rifier que les donnÃ©es sont bien sauvegardÃ©es
python -c "
from src.utils.sqlmanager import sql_manager
with sql_manager.get_connection() as conn:
    cursor = conn.cursor()
    cursor.execute('SHOW TABLES')
    tables = cursor.fetchall()
    print('Tables crÃ©Ã©es:', tables)
"
```

---

## ğŸ“š Documentation

### Collection Postman
Importez le fichier `POSTMAN_API.json` dans Postman pour tester tous les endpoints avec des exemples prÃ©-configurÃ©s.

### Swagger Interactive
AccÃ©dez Ã  http://localhost:8000/api/docs/ pour une documentation interactive avec tests en temps rÃ©el.

---

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes Courants

**âŒ Erreur de connexion MySQL :**
```bash
# VÃ©rifier que MySQL est dÃ©marrÃ©
sudo systemctl status mysql

# VÃ©rifier les credentials dans .env
```

**âŒ Erreur d'import Python :**
```bash
# VÃ©rifier que vous Ãªtes dans le bon rÃ©pertoire
pwd  # Doit afficher le dossier Challenge_DL_and_DI

# RÃ©installer les dÃ©pendances
pip install -r requirements.txt
```

**âŒ Ports occupÃ©s :**
```bash
# Changer le port Django si nÃ©cessaire
python manage.py runserver 8001
```

### Logs et Debug

```bash
# Activer les logs dÃ©taillÃ©s
export DJANGO_DEBUG=True

# VÃ©rifier les logs Spark
# Interface disponible sur http://localhost:4040 (si Spark actif)
```

---

## ğŸ¯ FonctionnalitÃ©s DÃ©monstrables

### 1. **Pipeline de DonnÃ©es Complet**
- âœ… Collecte automatisÃ©e depuis 6 sources
- âœ… Transformation avec validation qualitÃ©
- âœ… API REST avec 37 endpoints

### 2. **Analytics AvancÃ©es**
- âœ… Analyse multi-dimensionnelle (pays Ã— technologie Ã— temps)
- âœ… Scoring de qualitÃ© automatisÃ© (0-100)
- âœ… DÃ©tection de tendances et croissance

### 3. **Architecture Production**
- âœ… Gestion centralisÃ©e des connexions (pooling)
- âœ… Retry automatique et gestion d'erreurs
- âœ… Documentation Swagger complÃ¨te
- âœ… Tests Postman prÃ©-configurÃ©s

---

## ğŸš€ DÃ©mo Rapide

```bash
# 1. Installer et configurer (5 min)
git clone <repo> && cd Challenge_DL_and_DI
pip install -r requirements.txt
# CrÃ©er .env avec vos credentials MySQL

# 2. Collecter des donnÃ©es (2 min)
python src/bronze/import_github_repos.py

# 3. Transformer les donnÃ©es (1 min)  
python src/silver/process_github_repos.py

# 4. Lancer l'API (30 sec)
python manage.py runserver

# 5. Tester l'API (30 sec)
curl http://localhost:8000/api/github-repos/summary/
```

**RÃ©sultat** : API fonctionnelle avec donnÃ©es rÃ©elles en moins de 10 minutes ! ğŸ‰

---

> **ğŸ’¡ Note pour l'Ã‰valuation** : Ce projet dÃ©montre une maÃ®trise complÃ¨te du data engineering moderne avec architecture Medallion, APIs REST, et documentation production-ready. Les 4 sources principales sont entiÃ¨rement fonctionnelles pour une dÃ©monstration immÃ©diate.